# -*- coding: utf-8 -*-
"""MTP_Copy of S1 E1: Vision Language Modelling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i1BJwh0KHl0FVMcTXjm68pM99q28CScq
"""

# !pip install -qqq easy-vqa
# !pip install -qqq sentence_transformers transformers timm

"""## <font color="red"> 2. Dataset - EasyVQA </font>"""

# !git clone https://github.com/vzhou842/easy-VQA

# !python /content/easy-VQA/gen_data/generate_data.py
import logging

# Configure the logger
logging.basicConfig(filename='my_script.log', level=logging.INFO)

# Log messages
logging.info("Starting the script")

from easy_vqa import get_train_questions, get_test_questions

train_questions, train_answers, train_image_ids = get_train_questions()
test_questions, test_answers, test_image_ids = get_test_questions()

# Question 0 is at index 0 for all 3 arrays:
print(train_questions[0]) # what shape does the image contain?
print(train_answers[0])   # circle
print(train_image_ids[0]) # 0

from easy_vqa import get_train_image_paths, get_test_image_paths

train_image_paths = get_train_image_paths()
test_image_paths = get_test_image_paths()

print(train_image_paths[0]) # ends in easy_vqa/data/train/images/0.png

from easy_vqa import get_train_questions, get_test_questions

train_questions, train_answers, train_image_ids = get_train_questions()
test_questions, test_answers, test_image_ids = get_test_questions()

import pandas as pd
pd.set_option("max_colwidth", None)

def gen_dataframes(questions, answers, image_ids, mode="train"):
    records = []
    for question, answer, image_id in zip(questions, answers, image_ids):
        image_path = f"/DATA/pal14/M22MA002/EasyVqa/easy_vqa123/data/{mode}/images/{image_id}.png"
        records.append({"question" : question, "answer": answer, "image_path": image_path})
    return pd.DataFrame(records)

df =  gen_dataframes(train_questions, train_answers, train_image_ids)
from sklearn.model_selection import train_test_split
df = df.sample(frac=1)
train_df, eval_df = train_test_split(df)
test_df =  gen_dataframes(test_questions, test_answers, test_image_ids, mode="test")

print(train_df.shape)
print(eval_df.shape)
print(test_df.shape)

from easy_vqa import get_answers

answers = get_answers()
print("Total labels", len(answers))
label2idx = {answer:i for i, answer in enumerate(answers)}

label2idx

train_df["label"] = train_df["answer"].apply(lambda x: label2idx.get(x))
eval_df["label"] = eval_df["answer"].apply(lambda x: label2idx.get(x))
test_df["label"] = test_df["answer"].apply(lambda x: label2idx.get(x))

train_df.sample(5)

"""## <font color="red"> 3. A look at V+L Feature backbones </font>

**[Important]** <font color="blue">  A friendly reminder - The code might look less elegant to keep the feature backbones outside of the VQA model we are going to build. </font> but we are going to freeze all layers of the backbones and use them as pure feature extractors. So we don't need them as a part of our network. In the subsequent episodes as and when we need to finetune we will make the featue backbones as a part of the network. (If you dont follow this at the moment don't worry)
"""

from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModel
import torchvision.transforms as T
import torch
import timm


""" Fusing Transformers """
device = "cuda:0" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
text_encoder = AutoModel.from_pretrained("bert-base-uncased")
for p in text_encoder.parameters():
    p.requires_grad = False


image_processor = AutoFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")
image_encoder = AutoModel.from_pretrained("google/vit-base-patch16-224-in21k")

""" Fusing CNNs and Transformers """
# device = "cuda:0" if torch.cuda.is_available() else "cpu"
# image_encoder = timm.create_model("resnet50d", pretrained=True,  num_classes=0)
# resize_transform = T.Resize((224, 224))

for p in image_encoder.parameters():
    p.requires_grad = False


image_encoder.to(device)
text_encoder.to(device)

print()

"""## <font color="red"> 4 - Stitch torch dataset w/feature backbones </font>"""

from PIL import Image
from tqdm import tqdm
from torch import nn
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
from torchvision import transforms

class EasyQADataset(Dataset):

    def __init__(self,df,
                 image_encoder,
                 text_encoder,
                 image_processor,
                 tokenizer,
              ):
        self.df = df
        self.image_encoder = image_encoder
        self.text_encoder = text_encoder
        self.image_processor = image_processor
        self.tokenizer = tokenizer


    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):

        image_file = self.df["image_path"][idx]
        question = self.df['question'][idx]
        image = Image.open(image_file).convert("RGB")
        label = self.df['label'][idx]

        """ When CNNs are used for V backbone"""

        # image = resize_transform(image)
        # image_inputs = T.ToTensor()(image).unsqueeze_(0)
        # image_inputs = image_inputs.to(device)
        # image_outputs = self.image_encoder(image_inputs)
        # image_embedding = image_outputs[0]
        # image_embedding = image_embedding.detach()
        # print("Image emb", image_embedding.shape)

        """ When Transformers are used for V backbone"""
        image_inputs = self.image_processor(image, return_tensors="pt")
        image_inputs = {k:v.to(device) for k,v in image_inputs.items()}
        image_outputs = self.image_encoder(**image_inputs)
        image_embedding = image_outputs.pooler_output
        image_embedding = image_embedding.view(-1)
        image_embedding = image_embedding.detach()
        # print("Image emb", image_embedding.shape)

        text_inputs = self.tokenizer(question, return_tensors="pt")
        text_inputs = {k:v.to(device) for k,v in text_inputs.items()}
        text_outputs = self.text_encoder(**text_inputs)
        text_embedding = text_outputs.pooler_output # You can experiment with this or raw CLS embedding below
        #text_embedding = text_outputs.last_hidden_state[:,0,:] # Raw CLS embedding
        text_embedding = text_embedding.view(-1)
        text_embedding = text_embedding.detach()
        # print("Text emb", text_embedding.shape)

        encoding={}
        encoding["image_emb"] = image_embedding
        encoding["text_emb"] = text_embedding
        encoding["label"] = torch.tensor(label)

        return encoding

train_df.reset_index(drop=True, inplace=True)
eval_df.reset_index(drop=True, inplace=True)

train_dataset = EasyQADataset(
                           df=train_df,
                           image_encoder = image_encoder,
                           text_encoder = text_encoder,
                           tokenizer = tokenizer,
                           image_processor = image_processor, # Pass None when using CNNs
                           )

eval_dataset = EasyQADataset(
                           df=eval_df,
                           image_encoder = image_encoder,
                           text_encoder = text_encoder,
                           tokenizer = tokenizer,
                           image_processor = image_processor,# Pass None when using CNNs
                          )

from torch.utils.data import DataLoader, RandomSampler, SequentialSampler

batch_size = 32
eval_batch_size = 32
dataloader_train = DataLoader(train_dataset,
                              sampler=RandomSampler(train_dataset),
                              batch_size=batch_size)
dataloader_validation = DataLoader(eval_dataset,
                                   sampler=SequentialSampler(eval_dataset),
                                   batch_size=eval_batch_size)

from sklearn.metrics import accuracy_score

def accuracy_score_func(preds, labels):
    return accuracy_score(labels, preds)

"""## <font color="red"> 5. A Simple PyTorch Training loop </font>

**[Important]** <font color="blue">  Again, A friendly reminder - the code might look less elegant, but thats by choice. </font> In the subsequent episodes will move the loss handling inside the model forward().
"""

import random
from torch import nn
from tqdm.notebook import tqdm
import numpy as np
import requests

criterion = nn.CrossEntropyLoss()

def evaluate(dataloader_val):

    model.eval()

    loss_val_total = 0
    predictions, true_vals, confidence = [], [], []

    for batch in dataloader_val:

        batch = tuple(b.to(device) for b in batch.values())

        inputs = {'image_emb':  batch[0],'text_emb': batch[1]}

        with torch.no_grad():
            outputs = model(**inputs)

        labels =  batch[2]
        loss = criterion(outputs.view(-1, 13), labels.view(-1))
        loss_val_total += loss.item()

        probs   = torch.max(outputs.softmax(dim=1), dim=-1)[0].detach().cpu().numpy()
        outputs = outputs.argmax(-1)
        logits = outputs.detach().cpu().numpy()
        label_ids = labels.cpu().numpy()
        predictions.append(logits)
        true_vals.append(label_ids)
        confidence.append(probs)

    loss_val_avg = loss_val_total/len(dataloader_val)
    predictions = np.concatenate(predictions, axis=0)
    true_vals = np.concatenate(true_vals, axis=0)
    confidence = np.concatenate(confidence, axis=0)

    return loss_val_avg, predictions, true_vals, confidence

def train():

  train_history = open("/DATA/pal14/M22MA002/EasyVqa/models/train_history.csv", "w")
  log_hdr  = "Epoch, train_loss, train_acc, val_loss, val_acc"
  train_history.write(log_hdr  + "\n")
  train_f1s = []
  val_f1s = []
  train_losses = []
  val_losses = []
  min_val_loss = -1
  max_auc_score = 0
  epochs_no_improve = 0
  early_stopping_epoch = 3
  early_stop = False

  for epoch in tqdm(range(1, epochs+1)):

      model.train()
      loss_train_total = 0
      train_predictions, train_true_vals = [], []

      progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)

      for batch in progress_bar:
          model.zero_grad()
          batch = tuple(b.to(device) for b in batch.values())

          inputs = {'image_emb':  batch[0],'text_emb': batch[1]}
          labels =  batch[2]

          outputs = model(**inputs)
          loss = criterion(outputs.view(-1, 13), labels.view(-1))
          loss_train_total += loss.item()
          loss.backward()
          torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

          logits = outputs.argmax(-1)
          logits = logits.detach().cpu().numpy()
          label_ids = labels.cpu().numpy()
          train_predictions.append(logits)
          train_true_vals.append(label_ids)

          optimizer.step()
          scheduler.step()

          progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})



      train_predictions = np.concatenate(train_predictions, axis=0)
      train_true_vals = np.concatenate(train_true_vals, axis=0)

      tqdm.write(f'\nEpoch {epoch}')
      loss_train_avg = loss_train_total/len(dataloader_train)
      tqdm.write(f'Training loss: {loss_train_avg}')
      train_f1 = accuracy_score_func(train_predictions, train_true_vals)
      tqdm.write(f'Train Acc: {train_f1}')

      val_loss, predictions, true_vals,_ = evaluate(dataloader_validation)
      val_f1 = accuracy_score_func(predictions, true_vals)
      tqdm.write(f'Validation loss: {val_loss}')
      tqdm.write(f'Val Acc: {val_f1}')

      if val_f1 >= max_auc_score:
          tqdm.write('\nSaving best model')
          torch.save(model.state_dict(), f'/DATA/pal14/M22MA002/EasyVqa/easyvqa_finetuned_epoch_{epoch}.model')
          max_auc_score = val_f1

      train_losses.append(loss_train_avg)
      val_losses.append(val_loss)
      train_f1s.append(train_f1)
      val_f1s.append(val_f1)
      log_str  = "{}, {}, {}, {}, {}".format(epoch, loss_train_avg, train_f1, val_loss, val_f1)
      train_history.write(log_str + "\n")

      if min_val_loss < 0:
          min_val_loss = val_loss
      else:
        if val_loss < min_val_loss:
            min_val_loss = val_loss
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= early_stopping_epoch:
                early_stop = True
                break
            else:
                continue


  if early_stop:
    print("Early Stopping activated at epoch -", epoch )
    print("Use the checkpoint at epoch - ", epoch - early_stopping_epoch)

  train_history.close()
  return train_losses, val_losses

"""## <font color="red"> 6a. Model - An Early Fusion Network using CNN as V backbone and Transformers as L backbone </font>"""

class EasyQAEarlyFusionNetwork(nn.Module):

    def __init__(self, hyperparms=None):

        super(EasyQAEarlyFusionNetwork, self).__init__()
        self.dropout = nn.Dropout(0.3)
        self.vision_projection = nn.Linear(2048, 768)
        self.text_projection = nn.Linear(512, 768)
        self.fc1 = nn.Linear(768, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.classifier = nn.Linear(256, 13)
        W = torch.Tensor(768, 768)
        self.W = nn.Parameter(W)
        self.relu_f = nn.ReLU()
        # initialize weight matrices
        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))

    def forward(self, image_emb, text_emb):

        x1 = image_emb
        x1 = torch.nn.functional.normalize(x1, p=2, dim=1)
        Xv = self.relu_f(self.vision_projection(x1))

        x2 = text_emb
        x2 = torch.nn.functional.normalize(x2, p=2, dim=1)
        Xt = self.relu_f(self.text_projection(x2))

        Xvt = Xv * Xt
        Xvt = self.relu_f(torch.mm(Xvt, self.W.t()))

        Xvt = self.fc1(Xvt)
        Xvt = self.bn1(Xvt)
        Xvt = self.dropout(Xvt)
        Xvt = self.classifier(Xvt)

        return Xvt

"""## <font color="red"> 6b. Model - A Mid Fusion Network using Transformers as V and L backbones </font>"""

import math
class EasyQAMidFusionNetwork(nn.Module):

    def __init__(self, hyperparms=None):

        super(EasyQAMidFusionNetwork, self).__init__()
        self.dropout = nn.Dropout(0.3)
        self.fc1 = nn.Linear(768, 256)
        self.bn1 = nn.BatchNorm1d(256)
        self.classifier = nn.Linear(256, 13)
        W = torch.Tensor(768, 768)
        self.W = nn.Parameter(W)
        self.relu_f = nn.ReLU()
        # initialize weight matrices
        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))

    def forward(self, image_emb, text_emb):

        x1 = image_emb
        Xv = torch.nn.functional.normalize(x1, p=2, dim=1)

        x2 = text_emb
        Xt = torch.nn.functional.normalize(x2, p=2, dim=1)

        Xvt = Xv * Xt
        Xvt = self.relu_f(torch.mm(Xvt, self.W.t()))

        Xvt = self.fc1(Xvt)
        Xvt = self.bn1(Xvt)
        Xvt = self.dropout(Xvt)
        Xvt = self.classifier(Xvt)

        return Xvt

"""# <font color="red"> Terminology Hijack (or more directly Bastardization of Terminology): </font>

**A note on Early, Late and Mid Fusion**:

Traditionally in a pretransformer world (i.e. When vision was in pretranformer or CNN only era) Early Fusion referred to **feature fusion** and Late fusion referred to **decision fusion**

# <font color="red"> A recent Vision Language Pretraining review captures only 2 architecture styles, but writes-off custom fusion. </font>

# <font color="red"> Long list of *Almost* All VLP Models </font>
"""

torch.cuda.empty_cache()
model = EasyQAEarlyFusionNetwork()
model.to(device)
torch.cuda.empty_cache()
model = EasyQAMidFusionNetwork()
model.to(device)


from transformers import get_linear_schedule_with_warmup
from torch.optim import AdamW

optimizer = AdamW(model.parameters(),
                  lr=5e-5,
                  weight_decay = 1e-5,
                  eps=1e-8
                  )

epochs = 10
train_steps=20000
print("train_steps", train_steps)
warm_steps = train_steps * 0.1
print("warm_steps", warm_steps)
scheduler = get_linear_schedule_with_warmup(optimizer,
                                            num_warmup_steps=warm_steps,
                                            num_training_steps=train_steps)



"""# <font color="red"> 7a. Train the early fusion network. </font>


"""

from matplotlib import pyplot as plt

# !rm -rf /content/models
# !mkdir /content/models
print("fgf")
train_losses, val_losses =  train()
torch.cuda.empty_cache()
plt.plot(train_losses)
plt.plot(val_losses)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

import pickle

# Assuming 'model' is your trained machine learning model
# You can replace 'model' with your actual model object
model = model

# Specify the file path where you want to save the model
model_filepath = "/DATA/pal14/M22MA002/EasyVqa/models/my_modelcnn.pkl"  # Replace with your desired file path

# Save the model to a file using pickle
with open(model_filepath, 'wb') as model_file:
    pickle.dump(model, model_file)

"""# <font color="red"> 7b. Train the mid fusion network. </font>


"""




from matplotlib import pyplot as plt
# try:
# !rm -rf /content/models
# !mkdir /content/models
train_losses, val_losses =  train()
torch.cuda.empty_cache()
plt.plot(train_losses)
plt.plot(val_losses)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()
# except:
#     print("Sh*t happens !, if you are a self-respecting human, don't rote copy this. Catch the exception and write a nice message")

import pickle

# Assuming 'model' is your trained machine learning model
# You can replace 'model' with your actual model object
model = model

# Specify the file path where you want to save the model
model_filepath = "/DATA/pal14/M22MA002/EasyVqa/models/my_modelVit.pkl"  # Replace with your desired file path

# Save the model to a file using pickle
with open(model_filepath, 'wb') as model_file:
    pickle.dump(model, model_file)

"""# <font color="red"> 9. Run inference on the test data </font>"""
# from modelVQA import EasyQADataset
test_dataset = EasyQADataset(
                           df=test_df,
                           image_encoder = image_encoder,
                           text_encoder = text_encoder,
                           tokenizer = tokenizer,
                           image_processor = image_processor
                           )

device = "cuda:0"
model.load_state_dict(torch.load('/DATA/pal14/M22MA002/EasyVqa/models/easyvqa_finetuned_epoch_8.model'))
model.to(device)

dataloader_test = DataLoader(test_dataset,
                            sampler=SequentialSampler(test_dataset),
                            batch_size=128)

_, preds, truths, confidence = evaluate(dataloader_test)

"""# <font color="red"> 10. Moment of Truth :-)? </font>"""

print("Test Acc with Resnet50d: " , accuracy_score_func(preds,truths))

print("Test Acc with ViT: " , accuracy_score_func(preds,truths))

test_results_df = pd.concat([test_df, pd.DataFrame(preds, columns=["preds"]), pd.DataFrame(truths, columns=["gt"]), pd.DataFrame(confidence, columns=["confidence"])], axis=1)

test_results_df.sample(5)

"""# <font color="red"> Is the Model Overconfident when its wrong? </font>"""

test_results_df[(test_results_df["preds"] != test_results_df["gt"]) & (test_results_df["confidence"] >= 0.90)].shape

label2idx

from PIL import Image
Image.open("/DATA/pal14/M22MA002/EasyVqa/easy_vqa123/data/test/images/130.png")

logging.warning("This is a warning message")
logging.error("An error occurred")
